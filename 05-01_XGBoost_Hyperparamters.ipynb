{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.6 64-bit ('base': conda)"
    },
    "interpreter": {
      "hash": "016f416f43c7de9d71e7f049375c6b8b903fae571cb02d2ed6c16dadea121115"
    },
    "colab": {
      "name": "05-XGBoost Hyperparamters.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model 6\n",
        "using the best hyperparameters from the previous version. testing on new data.\n",
        "new data include new columns for each categorical column with a dominant category\n",
        "if the column value has the dominant value, then there is a 1 corresponding to \"isDominant\"\n",
        "other wise it is set to 0.\n",
        "Also adding a Linear Regression\n",
        "\n",
        "## Results:\n",
        "The linear regression shows no improvement on xgboost. \n",
        "With adding new data and applying feature engineering, the RMSE did not improve \n",
        "\n",
        "** RMSE Value 0.726979392992975**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# reading data from google drive \n",
        "# data paths whether notebook is run locally or google colab\n",
        "import os\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    COMPETETION_PATH = \"/content/drive/MyDrive/30-days-of-ml-competition1\"\n",
        "    TRAIN_DATA_PATH = \"/content/drive/MyDrive/30-days-of-ml-competition1/data/train.csv\"\n",
        "    TEST_DATA_PATH = \"/content/drive/MyDrive/30-days-of-ml-competition1/data/test.csv\"\n",
        "    OUTPUT_PATH = \"/content/drive/MyDrive/30-days-of-ml-competition1/output\"\n",
        "except:\n",
        "    TRAIN_DATA_PATH = os.path.join(\"data\", \"train.csv\")\n",
        "    TEST_DATA_PATH = os.path.join(\"data\", \"test.csv\")\n",
        "\n",
        "print(f\"Training Path {TRAIN_DATA_PATH}\")\n",
        "print(f\"Testing Path {TEST_DATA_PATH}\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Path data/train.csv\n",
            "Testing Path data/test.csv\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeZXk9xlGyYa",
        "outputId": "3ce477bc-c49f-402b-a292-7b1e3b5f6860"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "\n",
        "# library imports\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# modeling\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# evaluation\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ],
      "outputs": [],
      "metadata": {
        "id": "RWEQJbdUWEyJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "def read_organise_data(train=TRAIN_DATA_PATH):\n",
        "    \"\"\"read the data from a path and splitting to features and target\n",
        "\n",
        "    Args:\n",
        "        train (path, optional): The path of training data file to. Defaults to TRAIN_DATA_PATH.\n",
        "\n",
        "    Returns:\n",
        "        X, y: X for features and y for target\n",
        "    \"\"\"\n",
        "    full_df = pd.read_csv(train, index_col=\"id\")\n",
        "    print(f\"Shape of Dataset: {full_df.shape}\")\n",
        "    return full_df\n",
        "\n",
        "df= read_organise_data()\n",
        "X_test = pd.read_csv(TEST_DATA_PATH, index_col='id')\n",
        "print(f\"Shape of Test set {X_test.shape}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Dataset: (300000, 25)\n",
            "Shape of Test set (200000, 24)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU2jthoy9Cm6",
        "outputId": "a4b3898d-790c-4fea-9aa1-6a5fd49e6813"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Data"
      ],
      "metadata": {
        "id": "QIQ6r3X1WEyK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "y = df.target.copy()\n",
        "X = df.drop('target', axis=1).copy()"
      ],
      "outputs": [],
      "metadata": {
        "id": "zmCXXJzhWEyO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# splitting the data\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2,\n",
        "                                                      random_state= 1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "5pNen2jpWEyP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "\n",
        "# categorical columns start with cat\n",
        "cat_cnames = [cname for cname in X.columns if 'cat' in cname]\n",
        "# numerical columns starts with cont\n",
        "num_cnames = [cname for cname in X.columns if 'cont' in cname]\n",
        "total_cols = cat_cnames + num_cnames\n",
        "print(len(total_cols))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ],
      "metadata": {
        "id": "jfrTIecNWEyQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "\n",
        "# checking the cardinality of the categorical columns\n",
        "for cname in cat_cnames:\n",
        "    num_unique = X[cname].nunique()\n",
        "    print(f\"{cname} has {num_unique}\")\n",
        "    if num_unique > 10:\n",
        "        print(f\"\\t{cname} has a high cardinality\")\n",
        "\n",
        "# eventhough cat9 col has more than 10 unique values, we will still use the OnehotEncoder "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat0 has 2\n",
            "cat1 has 2\n",
            "cat2 has 2\n",
            "cat3 has 4\n",
            "cat4 has 4\n",
            "cat5 has 4\n",
            "cat6 has 8\n",
            "cat7 has 8\n",
            "cat8 has 7\n",
            "cat9 has 15\n",
            "\tcat9 has a high cardinality\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulfq78dIWEyQ",
        "outputId": "4895337a-b3b1-4405-b9d7-79b31c1994a6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "## columns with a dominant category\n",
        "# category columns with 1 value of more than 90% of data \n",
        "one_value_cat = []\n",
        "for cname in cat_cnames:\n",
        "    if X[cname].value_counts().iloc[0]/len(df) > 0.90:\n",
        "        one_value_cat.append(cname)\n",
        "\n",
        "one_value_cat"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cat4', 'cat6', 'cat7']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# adding a column corresponding to wethere a value is dominant or not\n",
        "# for each of the columns with 1 dominant value, add a column with the column name as a prefix\n",
        "# new columns: the values are 1 if the the row value of the column belongs to dominant category and 0 if the column value is not the dominant category\n",
        "# \n",
        "for cname in one_value_cat:\n",
        "    most_frequent_value = X[cname].value_counts().sort_values(ascending=False).index[0]\n",
        "    X[f'{cname}_isDominante'] = X[cname].apply(lambda x: 0 if x!=most_frequent_value else 1)\n",
        "    X_test[f'{cname}_isDominante'] = X_test[cname].apply(lambda x: 0 if x!=most_frequent_value else 1)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "X.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300000, 27)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "X_test.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000, 27)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "\n",
        "# numerical and categorical transformations\n",
        "num_transformer = StandardScaler()\n",
        "cat_transformer = OrdinalEncoder()\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', num_transformer, num_cnames),\n",
        "    ('cat', cat_transformer, cat_cnames),\n",
        "],remainder='passthrough')"
      ],
      "outputs": [],
      "metadata": {
        "id": "Y-1AbyL4Wh0o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "## pipeline\n",
        "\n",
        "best_model = XGBRegressor(n_estimators=1350, learning_rate=0.01, max_depth=3,\n",
        "                          objective='reg:squarederror', random_state=1)\n",
        "\n",
        "my_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', best_model)\n",
        "])\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "source": [
        "my_pipeline.fit(X_train, y_train)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('preprocessor',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='passthrough',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('num',\n",
              "                                                  StandardScaler(copy=True,\n",
              "                                                                 with_mean=True,\n",
              "                                                                 with_std=True),\n",
              "                                                  ['cont0', 'cont1', 'cont2',\n",
              "                                                   'cont3', 'cont4', 'cont5',\n",
              "                                                   'cont6', 'cont7', 'cont8',\n",
              "                                                   'cont9', 'cont10', 'cont11',\n",
              "                                                   'cont12', 'cont13']),\n",
              "                                                 ('cat',\n",
              "                                                  Ordi...\n",
              "                              interaction_constraints='', learning_rate=0.01,\n",
              "                              max_delta_step=0, max_depth=3, min_child_weight=1,\n",
              "                              missing=nan, monotone_constraints='()',\n",
              "                              n_estimators=1350, n_jobs=0, num_parallel_tree=1,\n",
              "                              objective='reg:squarederror', random_state=1,\n",
              "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "                              subsample=1, tree_method='exact',\n",
              "                              validate_parameters=1, verbosity=None))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "source": [
        "\n",
        "prediction_valid = my_pipeline.predict(X_valid)\n",
        "print(mean_squared_error(y_valid, prediction_valid)**(1/2))\n",
        "# old value before adding new columns prediction_valid = my_pipeline.predict(X_valid)\n",
        "#print(mean_squared_error(y_valid, prediction_valid)**(1/2))\n",
        "# old value before adding new columns 0.7178614528083396"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.726979392992975\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "predictions = best_pipeline.predict(X_test)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "OUTPUT_PATH = 'output'\n",
        "def output_submission(prediction, file_name):\n",
        "    \"\"\"creating a kaggle submission file\n",
        "\n",
        "    Args:\n",
        "        prediction (array): an array of predictions of the test dataset\n",
        "        file_name (string): a string for the name without the extension\n",
        "    \"\"\"\n",
        "    my_submission = pd.DataFrame({'target': predictions},\n",
        "                                 index=X_test.index)\n",
        "    #my_submission.set_index('id')\n",
        "    file_path = os.path.join(OUTPUT_PATH,file_name)\n",
        "    my_submission.to_csv(f'{file_path}.csv')\n",
        "    print(f'A submission file has been made at {file_path}')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "source": [
        "output_submission(predictions, \"Submission10\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A submission file has been made at output/Submission10\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# linear regression pipeline\n",
        "linear_model = LinearRegression()\n",
        "linear_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', linear_model),\n",
        "])"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "linear_pipeline.fit(X_train, y_train)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('preprocessor',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='passthrough',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('num',\n",
              "                                                  StandardScaler(copy=True,\n",
              "                                                                 with_mean=True,\n",
              "                                                                 with_std=True),\n",
              "                                                  ['cont0', 'cont1', 'cont2',\n",
              "                                                   'cont3', 'cont4', 'cont5',\n",
              "                                                   'cont6', 'cont7', 'cont8',\n",
              "                                                   'cont9', 'cont10', 'cont11',\n",
              "                                                   'cont12', 'cont13']),\n",
              "                                                 ('cat',\n",
              "                                                  OrdinalEncoder(categories='auto',\n",
              "                                                                 dtype=<class 'numpy.float64'>),\n",
              "                                                  ['cat0', 'cat1', 'cat2',\n",
              "                                                   'cat3', 'cat4', 'cat5',\n",
              "                                                   'cat6', 'cat7', 'cat8',\n",
              "                                                   'cat9'])],\n",
              "                                   verbose=False)),\n",
              "                ('model',\n",
              "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
              "                                  normalize=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "mean_squared_error(y_valid, linear_pipeline.predict(X_valid))**(1/2)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7372876297579729"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "y2 = df.target.copy()\n",
        "X2 = df.drop('target', axis=1).copy()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "X_train2, X_valid2, y_train2, y_valid2 = train_test_split(X2, y2, test_size=0.2, random_state=1)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "## pipeline\n",
        "\n",
        "best_model2 = XGBRegressor(n_estimators=1350, learning_rate=0.01, max_depth=3,\n",
        "                          objective='reg:squarederror', random_state=1)\n",
        "\n",
        "my_pipeline2 = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', best_model2)\n",
        "])\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "my_pipeline2.fit(X_train2, y_train2)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('preprocessor',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='passthrough',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('num',\n",
              "                                                  StandardScaler(copy=True,\n",
              "                                                                 with_mean=True,\n",
              "                                                                 with_std=True),\n",
              "                                                  ['cont0', 'cont1', 'cont2',\n",
              "                                                   'cont3', 'cont4', 'cont5',\n",
              "                                                   'cont6', 'cont7', 'cont8',\n",
              "                                                   'cont9', 'cont10', 'cont11',\n",
              "                                                   'cont12', 'cont13']),\n",
              "                                                 ('cat',\n",
              "                                                  Ordi...\n",
              "                              interaction_constraints='', learning_rate=0.01,\n",
              "                              max_delta_step=0, max_depth=3, min_child_weight=1,\n",
              "                              missing=nan, monotone_constraints='()',\n",
              "                              n_estimators=1350, n_jobs=0, num_parallel_tree=1,\n",
              "                              objective='reg:squarederror', random_state=1,\n",
              "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "                              subsample=1, tree_method='exact',\n",
              "                              validate_parameters=1, verbosity=None))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "xgb_predictions2 = my_pipeline2.predict(X_valid2)\n",
        "rmse = mean_squared_error(y_valid2, xgb_predictions2)\n",
        "print(f\"Root Mean Squeared Error {rmse**(1/2)}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squeared Error 0.726979392992975\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "# linear regression pipeline\n",
        "linear_model2 = LinearRegression()\n",
        "linear_pipeline2 = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', linear_model2),\n",
        "])\n",
        "linear_pipeline2.fit(X_train2, y_train2)\n",
        "linear_predictions2 = linear_pipeline2.predict(X_valid2)\n",
        "linear_rmse = mean_squared_error(y_valid2, linear_predictions2)\n",
        "print(f\"Linear Model on original data: {linear_rmse**(1/2)}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Model on original data: 0.7372876297579729\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "X.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300000, 27)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "X2.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300000, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    }
  ]
}